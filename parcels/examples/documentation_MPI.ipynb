{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial on Parallel Parcels with MPI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parcels can be run in Parallel with MPI. To do this, first follow the installation instructions at http://oceanparcels.org/#parallel_install."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the ParticleSet with MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have installed a parallel version of Parcels, you can simply run your script with\n",
    "\n",
    "```\n",
    "mpirun -np <np> python <yourscript.py>\n",
    "```\n",
    "Where `<np>` is the number of processors you want to use\n",
    "\n",
    "Parcels will then split the `ParticleSet` into `<np>` smaller ParticleSets, based on a `sklearn.cluster.KMeans` clustering. Each of those smaller `ParticleSets` will be executed by one of the `<np>` MPI processors.\n",
    "\n",
    "Note that in principle this means that all MPI processors need access to the full `FieldSet`, which can be Gigabytes in size for large global datasets. Therefore, efficient parallelisation only works if at the same time we also chunk the `FieldSet` into smaller domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking the FieldSet with dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of field-chunking in Parcels is that we use the `dask` library to load only these regions of the `Field` that are occupied by `Particles`. The advantage is that if different MPI procesors take care of Particles in different parts of the domain, each only needs to load a small section of the full `FieldSet` (although note that this load-balancing functionality is still in [development](#Future-developments)). Furthermore, the field-chunking in principle makes the `indices` keyword superfluous, as Parcels will determine which part of the domain to load itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default behaviour is for `dask` to control the chunking, via a call to `da.from_array(data, chunks='auto')`. The chunksizes are then determined by the layout of the NetCDF files. \n",
    "\n",
    "However, in tests we have experienced that this may not necessarily be the most efficient chunking. Therefore, Parcels provides control over the chunk-size via the `field_chunksize` keyword in `Field` creation, which requires a tuple that sets the typical size of chunks for each dimension.\n",
    "\n",
    "It is strongly encouraged to explore what the best value for chunksize is for your experiment, which will depend  on the `FieldSet`, the `ParticleSet` and the type of simulation (2D versus 3D). As a guidance, we have found that chunksizes in the zonal and meridional direction of approximately around *128-512* are typically most effective. The binning relates to the size of the model and its data size, so power-of-two values are advantageous but not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook below shows an example script to explore the scaling of the time taken for `pset.execute()` as a function of zonal and meridional `field_chunksize` for a dataset from the [CMEMS](http://marine.copernicus.eu/) portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "WARNING: Casting depth data to np.float32\n",
      "INFO: Compiled JITParticleAdvectionRK4 ==> /tmp/parcels-1000/aaceb54d27d80e479c28bd38b5e708fa_0.so\n",
      "100% (2592000.0 of 2592000.0) |##########| Elapsed Time: 0:54:56 Time:  0:54:56\n",
      "INFO: Compiled JITParticleAdvectionRK4 ==> /tmp/parcels-1000/72596263523efd3a43fee154f8cd9f58_0.so\n",
      " 16% (428400.0 of 2592000.0) |#          | Elapsed Time: 0:03:24 ETA:   0:19:19"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "%pylab inline\n",
    "from parcels import FieldSet, ParticleSet, JITParticle, AdvectionRK4\n",
    "from datetime import timedelta as delta\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def set_cmems_fieldset(cs):\n",
    "    data_dir_head = \"/data\"\n",
    "    data_dir = os.path.join(data_dir_head, \"CMEMS/GLOBAL_REANALYSIS_PHY_001_030/\")\n",
    "    files = sorted(glob(data_dir+\"mercatorglorys12v1_gl12_mean_201607*.nc\"))\n",
    "    variables = {'U': 'uo', 'V': 'vo'}\n",
    "    dimensions = {'lon': 'longitude', 'lat': 'latitude', 'time': 'time'}\n",
    "\n",
    "    if cs not in ['auto', False]:\n",
    "        cs = (1, cs, cs)\n",
    "    return FieldSet.from_netcdf(files, variables, dimensions, field_chunksize=cs)\n",
    "\n",
    "func_time = []\n",
    "mem_used_GB = []\n",
    "chunksize = [50, 100, 200, 400, 800, 1000, 1500, 2000, 2500, 4000, 'auto', False]\n",
    "for cs in chunksize:\n",
    "    \n",
    "    fieldset = set_cmems_fieldset(cs)\n",
    "    pset = ParticleSet(fieldset=fieldset, pclass=JITParticle, lon=[0], lat=[0], repeatdt=delta(hours=1))\n",
    "\n",
    "    tic = time.time()\n",
    "    pset.execute(AdvectionRK4, dt=delta(hours=1))\n",
    "    func_time.append(time.time()-tic)\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_B_used = process.memory_info().rss\n",
    "    mem_used_GB.append(mem_B_used / (1024 * 1024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 7)) \n",
    "\n",
    "ax.plot(chunksize[:-2], func_time[:-2], 'o-')\n",
    "ax.plot([0, 4300], [func_time[-2], func_time[-2]], '--', label=chunksize[-2])\n",
    "ax.plot([0, 4300], [func_time[-1], func_time[-1]], '--', label=chunksize[-1])\n",
    "plt.xlim([0, 4300])\n",
    "plt.legend()\n",
    "ax.set_xlabel('field_chunksize')\n",
    "ax.set_ylabel('Time spent in pset.execute() [s]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows that in this case, `field_chunksize='auto'` and `field_chunksize=False` (the two dashed lines) are roughly the same speed, but that the fastest run is for `field_chunksize=(1, 400, 400)`. \n",
    "\n",
    "Important to note is that for small chunksize (`field_chunksize=(1, 50, 50)` in this case), the execution time is actually _longer_ than for no chunking at all, due to `dask`- and memory management overhead. The actual thresholds and numbers depend on the `FieldSet` used and the specifics of your experiment.\n",
    "\n",
    "Also note that this is for a 2D application. For 3D applications, the `field_chunksize=False` will almost always be slower than `field_chunksize='auto'` or any tuple. That can be seen in the plot below, which shows the same analysis but then for a set of simulations using the full 3D CMEMS code. In this case, the `field_chunksize='auto'` is about a factor 100(!!) faster than running without chunking. But choosing too small chunksizes can make the code even slower, again highlighting that it is wise to explore which chunksize is best for your experiment before you perform it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 7)) \n",
    "\n",
    "ax.plot(chunksize[:-2], func_time3D[:-2], 'o-')\n",
    "ax.plot([0, 4300], [func_time3D[-2], func_time3D[-2]], '--', label=chunksize[-2])\n",
    "ax.plot([0, 4300], [func_time3D[-1], func_time3D[-1]], '--', label=chunksize[-1])\n",
    "plt.xlim([0, 4300])\n",
    "plt.legend()\n",
    "ax.set_xlabel('field_chunksize')\n",
    "ax.set_ylabel('Time spent in pset.execute() [s]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, one of the major advantages of field chunking is the efficient utilization of memory. This improvement in memory utilization in the end permits the distribution of the particle advection to many cores, as otherwise the processing unit (e.g. a CPU core; a node in a cluster) would exhaust the memory rapidly. This is shown in the following plot of the memory behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "ax.plot(chunksize[:-2], mem_used_GB[:-2], '--', label=\"memory_blocked [MB]\")\n",
    "ax.plot([0, 4000], [mem_used_GB[-2], mem_used_GB[-2]], 'x-', label=\"auto [MB]\")\n",
    "ax.plot([0, 4000], [mem_used_GB[-1], mem_used_GB[-1]], '--', label=\"no chunking [MB]\")\n",
    "plt.legend()\n",
    "ax.set_xlabel('field_chunksize')\n",
    "ax.set_ylabel('Memory blocked in pset.execute() [MB]')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can clearly see that with `field_chunksize=False`, all field data are loaded in full directly into memory, which can lead to MPI-run simulations being aborted for memory reasons. Furthermore, one can see that even the automatic method is not to optimal, and optimizing the chunk size for a specific oceanic model can make a big difference. It may be - depending on your simulation goal - even necessary to tweak the chunk size to have more memory space for additional particles that are being simulated. After all, particles and fields share the same memory space, so the less memory is consumed by zour field model, the more space there is for particles that are being simulated. \n",
    "\n",
    "### Future developments: load-balancing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current implementation of MPI parallelisations in Parcels is still fairly rudimentary. In particular, we will continue to develop the load-balancing of the `ParticleSet`.\n",
    "\n",
    "With load-balancing we mean that `Particles` that are close together are ideally on the same MPI processor. Practically, it means that we need to take care how `Particles` are spread over chunks and processors. See for example the two figures below:\n",
    "\n",
    "![](http://oceanparcels.org/images/parcelsParallel.png)\n",
    "*Example of load-balancing for Particles. The domain is chunked along the thick lines, and the orange and blue particles are on separate MPI processors. Before load-balancing (left panel), two chuncks in the centre of the domain have both orange and blue particles. After the load-balancing (right panel), the Particles are redistributed over the processors so that the number of chunks and particles per processor is optimised.*\n",
    "\n",
    "The difficulty is that since we don't know how the `ParticleSet` will disperse over time, we need to do this load-balancing 'on the fly'. If you to contribute to the optimisation of the load-balancing, please leave a message on [github](https://github.com/OceanParcels/parcels/issues)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}